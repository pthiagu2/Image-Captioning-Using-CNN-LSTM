{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.python.platform\n",
    "import pickle as pkl\n",
    "import os\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import cv2\n",
    "#import skimage\n",
    "from keras.preprocessing import sequence\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_path = './models/tensorflow'\n",
    "model_path_transfer = './models/tf_final'\n",
    "#Features of VGG 16\n",
    "feature_path = './data/feats.npy'\n",
    "#Captions for images\n",
    "annotation_path = './data/results_20130124.token'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_input(annotation_path, feature_path):\n",
    "     annotations = pd.read_table(annotation_path, sep='\\t', header=None, names=['image', 'caption'])\n",
    "     features=np.load(feature_path,'r')\n",
    "     capt=annotations['caption'].values\n",
    "     return features,capt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats, captions = data_input(annotation_path,feature_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(158915, 4096)\n",
      "(158915,)\n"
     ]
    }
   ],
   "source": [
    "print(feats.shape)\n",
    "print(captions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two young guys with shaggy hair look at their hands while hanging out in the yard .\n"
     ]
    }
   ],
   "source": [
    "print(captions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_data(sentence, max_word_count=30): # function from Andre Karpathy's NeuralTalk\n",
    "    print('preprocessing %d word vocab' % (max_word_count, ))\n",
    "    no_of_words = {}\n",
    "    nsents = 0\n",
    "    for s in sentence:\n",
    "      nsents += 1\n",
    "      for w in s.lower().split(' '):\n",
    "        no_of_words[w] = no_of_words.get(w, 0) + 1\n",
    "    vocab = [w for w in no_of_words if no_of_words[w] >= max_word_count]\n",
    "    print('preprocessed words %d -> %d' % (len(no_of_words), len(vocab)))\n",
    "\n",
    "    ixtoword = {}\n",
    "    ixtoword[0] = '.'  \n",
    "    wordtoix = {}\n",
    "    wordtoix['#START#'] = 0 \n",
    "    ix = 1\n",
    "    for w in vocab:\n",
    "      wordtoix[w] = ix\n",
    "      ixtoword[ix] = w\n",
    "      ix += 1\n",
    "\n",
    "    no_of_words['.'] = nsents\n",
    "    bias_init_vector = np.array([1.0*no_of_words[ixtoword[i]] for i in ixtoword])\n",
    "    bias_init_vector /= np.sum(bias_init_vector) \n",
    "    bias_init_vector = np.log(bias_init_vector)\n",
    "    bias_init_vector -= np.max(bias_init_vector) \n",
    "    return wordtoix, ixtoword, bias_init_vector.astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Caption_Generator():\n",
    "    def __init__(self, input_dimension, dim_embed, hidden_dimension, batch_size, num_lstm_steps, n_words, init_b):\n",
    "\n",
    "        self.input_dimension = input_dimension\n",
    "        self.dim_embed = dim_embed\n",
    "        self.hidden_dimension = hidden_dimension\n",
    "        self.batch_size = batch_size\n",
    "        self.num_lstm_steps = num_lstm_steps\n",
    "        self.n_words = n_words\n",
    "        \n",
    "        # declare the variables to be used for our word embeddings\n",
    "        with tf.device(\"/cpu:0\"):\n",
    "            self.word_embedding = tf.Variable(tf.random_uniform([self.n_words, self.dim_embed], -0.1, 0.1))\n",
    "\n",
    "        self.embedding_bias = tf.Variable(tf.zeros([dim_embed]))\n",
    "        \n",
    "        # declare the LSTM itself\n",
    "        self.lstm = tf.contrib.rnn.BasicLSTMCell(hidden_dimension)\n",
    "        \n",
    "        # declare the variables to be used to embed the image feature embedding to the word embedding space\n",
    "        self.embedded_img = tf.Variable(tf.random_uniform([input_dimension, hidden_dimension], -0.1, 0.1))\n",
    "        self.embedded_img_bias = tf.Variable(tf.zeros([hidden_dimension]))\n",
    "\n",
    "        # declare the variables to go from an LSTM output to a word encoding output\n",
    "        self.encoded_word = tf.Variable(tf.random_uniform([hidden_dimension, n_words], -0.1, 0.1))\n",
    "        # initialize this bias variable from the preprocess_data output\n",
    "        self.encoded_word_bias = tf.Variable(init_b)\n",
    "\n",
    "    def build_model(self):\n",
    "        # declaring the placeholders for our extracted image feature vectors, our caption, and our mask\n",
    "        # (describes how long our caption is with an array of 0/1 values of length `maxlen`  \n",
    "        img = tf.placeholder(tf.float32, [self.batch_size, self.input_dimension])\n",
    "        caption_placeholder = tf.placeholder(tf.int32, [self.batch_size, self.num_lstm_steps])\n",
    "        mask = tf.placeholder(tf.float32, [self.batch_size, self.num_lstm_steps])\n",
    "        \n",
    "        # getting an initial LSTM embedding from our image_imbedding\n",
    "        image_embedding = tf.matmul(img, self.embedded_img) + self.embedded_img_bias\n",
    "        \n",
    "        # setting initial state of our LSTM\n",
    "        state = self.lstm.zero_state(self.batch_size, dtype=tf.float32)\n",
    "\n",
    "        total_loss = 0.0\n",
    "        with tf.variable_scope(\"RNN\"):\n",
    "            for i in range(self.num_lstm_steps): \n",
    "                if i > 0:\n",
    "                   #if this isnâ€™t the first iteration of our LSTM we need to get the word_embedding corresponding\n",
    "                   # to the (i-1)th word in our caption \n",
    "                    with tf.device(\"/cpu:0\"):\n",
    "                        current_embedding = tf.nn.embedding_lookup(self.word_embedding, caption_placeholder[:,i-1]) + self.embedding_bias\n",
    "                else:\n",
    "                     #if this is the first iteration of our LSTM we utilize the embedded image as our input \n",
    "                    current_embedding = image_embedding\n",
    "                if i > 0: \n",
    "                    # allows us to reuse the LSTM tensor variable on each iteration\n",
    "                    tf.get_variable_scope().reuse_variables()\n",
    "\n",
    "                out, state = self.lstm(current_embedding, state)\n",
    "\n",
    "                \n",
    "                if i > 0:\n",
    "                    #get the one-hot representation of the next word in our caption \n",
    "                    labels = tf.expand_dims(caption_placeholder[:, i], 1)\n",
    "                    ix_range=tf.range(0, self.batch_size, 1)\n",
    "                    ixs = tf.expand_dims(ix_range, 1)\n",
    "                    concat = tf.concat([ixs, labels],1)\n",
    "                    onehot = tf.sparse_to_dense(\n",
    "                            concat, tf.stack([self.batch_size, self.n_words]), 1.0, 0.0)\n",
    "\n",
    "\n",
    "                    #perform a softmax classification to generate the next word in the caption\n",
    "                    logit = tf.matmul(out, self.encoded_word) + self.encoded_word_bias\n",
    "                    loss_cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logit, labels=onehot)\n",
    "                    loss_cross_entropy = loss_cross_entropy * mask[:,i]\n",
    "\n",
    "                    loss = tf.reduce_sum(loss_cross_entropy)\n",
    "                    total_loss += loss\n",
    "\n",
    "            total_loss = total_loss / tf.reduce_sum(mask[:,1:])\n",
    "            return total_loss, img,  caption_placeholder, mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "### Parameters ###\n",
    "dim_embed = 256\n",
    "hidden_dimension = 256\n",
    "input_dimension = 4096\n",
    "batch_size = 128\n",
    "momentum = 0.9\n",
    "n_epochs = 150\n",
    "\n",
    "def train(learning_rate=0.001, continue_training=False, transfer=True):\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    feats, captions = data_input(annotation_path, feature_path)\n",
    "    wordtoix, ixtoword, init_b = preprocess_data(captions)\n",
    "\n",
    "    np.save('data/ixtoword', ixtoword)\n",
    "\n",
    "    index = (np.arange(len(feats)).astype(int))\n",
    "    np.random.shuffle(index)\n",
    "\n",
    "\n",
    "    sess = tf.InteractiveSession()\n",
    "    n_words = len(wordtoix)\n",
    "    maxlen = np.max( [x for x in map(lambda x: len(x.split(' ')), captions) ] )\n",
    "    caption_generator = Caption_Generator(input_dimension, hidden_dimension, dim_embed, batch_size, maxlen+2, n_words, init_b)\n",
    "\n",
    "    loss, image, sentence, mask = caption_generator.build_model()\n",
    "\n",
    "    saver = tf.train.Saver(max_to_keep=100)\n",
    "    global_step=tf.Variable(0,trainable=False)\n",
    "    learning_rate = tf.train.exponential_decay(learning_rate, global_step,\n",
    "                                       int(len(index)/batch_size), 0.95)\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "    tf.global_variables_initializer().run()\n",
    "\n",
    "    if continue_training:\n",
    "        if not transfer:\n",
    "            saver.restore(sess,tf.train.latest_checkpoint(model_path))\n",
    "        else:\n",
    "            saver.restore(sess,tf.train.latest_checkpoint(model_path_transfer))\n",
    "    losses=[]\n",
    "    for epoch in range(n_epochs):\n",
    "        for start, end in zip( range(0, len(index), batch_size), range(batch_size, len(index), batch_size)):\n",
    "\n",
    "            current_feats = feats[index[start:end]]\n",
    "            current_captions = captions[index[start:end]]\n",
    "            current_caption_ind = [x for x in map(lambda cap: [wordtoix[word] for word in cap.lower().split(' ')[:-1] if word in wordtoix], current_captions)]\n",
    "\n",
    "            current_caption_matrix = sequence.pad_sequences(current_caption_ind, padding='post', maxlen=maxlen+1)\n",
    "            current_caption_matrix = np.hstack( [np.full( (len(current_caption_matrix),1), 0), current_caption_matrix] )\n",
    "\n",
    "            current_mask_matrix = np.zeros((current_caption_matrix.shape[0], current_caption_matrix.shape[1]))\n",
    "            nonzeros = np.array([x for x in map(lambda x: (x != 0).sum()+2, current_caption_matrix )])\n",
    "\n",
    "            for ind, row in enumerate(current_mask_matrix):\n",
    "                row[:nonzeros[ind]] = 1\n",
    "\n",
    "            _, loss_value = sess.run([train_op, loss], feed_dict={\n",
    "                image: current_feats.astype(np.float32),\n",
    "                sentence : current_caption_matrix.astype(np.int32),\n",
    "                mask : current_mask_matrix.astype(np.float32)\n",
    "                })\n",
    "\n",
    "            print(\"Current Cost: \", loss_value, \"\\t Epoch {}/{}\".format(epoch, n_epochs), \"\\t Iter {}/{}\".format(start,len(feats)))\n",
    "        print(\"Saving the model from epoch: \", epoch)\n",
    "        saver.save(sess, os.path.join(model_path, 'model'), global_step=epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing 30 word vocab\n",
      "preprocessed words 20326 -> 2942\n",
      "Current Cost:  5.46115 \t Epoch 0/150 \t Iter 0/158915\n",
      "Current Cost:  5.46305 \t Epoch 0/150 \t Iter 128/158915\n",
      "Current Cost:  5.30741 \t Epoch 0/150 \t Iter 256/158915\n",
      "Current Cost:  5.32423 \t Epoch 0/150 \t Iter 384/158915\n",
      "Current Cost:  5.35012 \t Epoch 0/150 \t Iter 512/158915\n",
      "Current Cost:  5.3583 \t Epoch 0/150 \t Iter 640/158915\n",
      "Current Cost:  5.25105 \t Epoch 0/150 \t Iter 768/158915\n",
      "Current Cost:  5.29672 \t Epoch 0/150 \t Iter 896/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 1024/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 1152/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 1280/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 1408/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 1536/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 1664/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 1792/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 1920/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 2048/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 2176/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 2304/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 2432/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 2560/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 2688/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 2816/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 2944/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 3072/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 3200/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 3328/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 3456/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 3584/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 3712/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 3840/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 3968/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 4096/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 4224/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 4352/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 4480/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 4608/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 4736/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 4864/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 4992/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 5120/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 5248/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 5376/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 5504/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 5632/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 5760/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 5888/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 6016/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 6144/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 6272/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 6400/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 6528/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 6656/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 6784/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 6912/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 7040/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 7168/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 7296/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 7424/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 7552/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 7680/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 7808/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 7936/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 8064/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 8192/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 8320/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 8448/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 8576/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 8704/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 8832/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 8960/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 9088/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 9216/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 9344/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 9472/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 9600/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 9728/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 9856/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 9984/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 10112/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 10240/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 10368/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 10496/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 10624/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 10752/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 10880/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 11008/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 11136/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 11264/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 11392/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 11520/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 11648/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 11776/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 11904/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 12032/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 12160/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 12288/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 12416/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 12544/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 12672/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 12800/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 12928/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 13056/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 13184/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 13312/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 13440/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 13568/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 13696/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 13824/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 13952/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 14080/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 14208/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 14336/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 14464/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 14592/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 14720/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 14848/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 14976/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 15104/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 15232/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 15360/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 15488/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 15616/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 15744/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 15872/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 16000/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 16128/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 16256/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 16384/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 16512/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 16640/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 16768/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 16896/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 17024/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 17152/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 17280/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 17408/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 17536/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 17664/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 17792/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 17920/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 18048/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 18176/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 18304/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 18432/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 18560/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 18688/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 18816/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 18944/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 19072/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 19200/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 19328/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 19456/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 19584/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 19712/158915\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Cost:  nan \t Epoch 0/150 \t Iter 19840/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 19968/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 20096/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 20224/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 20352/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 20480/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 20608/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 20736/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 20864/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 20992/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 21120/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 21248/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 21376/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 21504/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 21632/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 21760/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 21888/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 22016/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 22144/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 22272/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 22400/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 22528/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 22656/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 22784/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 22912/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 23040/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 23168/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 23296/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 23424/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 23552/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 23680/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 23808/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 23936/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 24064/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 24192/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 24320/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 24448/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 24576/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 24704/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 24832/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 24960/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 25088/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 25216/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 25344/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 25472/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 25600/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 25728/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 25856/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 25984/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 26112/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 26240/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 26368/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 26496/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 26624/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 26752/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 26880/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 27008/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 27136/158915\n",
      "Current Cost:  nan \t Epoch 0/150 \t Iter 27264/158915\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    train(.001,False,False) #train from scratch\n",
    "except KeyboardInterrupt:\n",
    "    print('Exiting Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
